# Проектный практикум. Учебная задача. Мониторинг экосистемы через IT-решения

## Описание проекта

Проект демонстрирует, как с помощью компьютерного зрения и предобученной нейронной сети можно автоматизировать мониторинг состояния экосистемы по данным визуального контроля (в этом конкретном проекте, по кадрам с дронов).

В ноутбуке `Мониторинг экосистемы через IT-решения.ipynb` реализован полный цикл:
- исследовательский анализ структуры датасета с изображениями мусора;
- подготовка данных и генерация `DataLoader` для выборок train/val/test;
- дообучение предобученной модели ResNet18 под 5 классов мусора;
- оценка качества (accuracy, матрица ошибок, `classification_report`);
- визуализация предсказаний модели;
- сохранение обученной модели в файл и проверки работоспособности модели после повторной загрузки.

Ноутбук демонстрирует учебный пример построения IT-решения для экологического мониторинга, на основе методов компьютерного зрения. В данной реализации заморожены внутренние слои ResNet18, а обучение ведётся только для нового выходного полносвязного слоя.

---

## Основные цели

1. **Подготовка и анализ данных визуального контроля**
   - Организация датасета изображений по классам и подвыборкам (train, val, test).
   - Подсчёт количества изображений в разрезе классов и выборок.
   - Визуальный разведочный анализ распределений классов.

2. **Построение и дообучение модели компьютерного зрения**
   - Использование предобученной модели ResNet18 из `torchvision.models`.
   - Замена выходного слоя под 5 целевых классов.
   - Настройка оптимизатора `Adam` и функции потерь `CrossEntropyLoss`.
   - Обучение модели на тренировочной выборке с валидацией по эпохам на валидационной выборке.

3. **Оценка качества и интерпретация результатов**
   - Получение предсказаний на тестовом наборе.
   - Расчёт метрик: accuracy, precision, recall, F1-score по каждому классу.
   - Построение обычной и нормированной матрицы ошибок.
   - Визуализация примеров изображений с истинными и предсказанными классами.

4. **Подготовка прототипа для интеграции в IT-систему мониторинга**
   - Сохранение обученной модели в файл.
   - Загрузка сохранённых весов в новый экземпляр модели и проверка работоспособности с целью демонстрации метода возможной интеграции модели в систему мониторинга.

---

## Датасет и предметная область

В качестве приближённой задачи мониторинга экосистемы рассматривается классификация объектов мусора по изображениям. Это может соответствовать сценарию:

- дрон или камера собирает изображения территории;
- модель определяет типы обнаруженных отходов;
- далее информация используется для анализа загрязнения и планирования мероприятий по уборке и профилактике.

Используются 5 классов мусора:

- `glass` — стекло;
- `plastic_bottle_takeaway_cup` — пластиковые бутылки и одноразовые стаканы;
- `retort_pouch` — пищевая упаковка;
- `take_away_container` — ланч-боксы и контейнеры;
- `tin_aluminium_cans` — жестяные и алюминиевые банки.

Ожидаемая структура папок с данными:

```text
datasets/
├── train/
│   ├── glass/
│   ├── plastic_bottle_takeaway_cup/
│   ├── retort_pouch/
│   ├── take_away_container/
│   └── tin_aluminium_cans/
├── val/
│   ├── glass/
│   ├── plastic_bottle_takeaway_cup/
│   ├── retort_pouch/
│   ├── take_away_container/
│   └── tin_aluminium_cans/
└── test/
    ├── glass/
    ├── plastic_bottle_takeaway_cup/
    ├── retort_pouch/
    ├── take_away_container/
    └── tin_aluminium_cans/
```

Внутри каждой папки класса находятся изображения в формате JPEG.

---

## Используемые технологии

Язык и окружение:

- Python
- Jupyter Notebook

Библиотеки для ML и CV:

- `torch`, `torch.nn`, `torch.optim`
- `torchvision.models` (ResNet18)
- `torchvision.transforms` (`Compose`, `Resize`, `ToTensor`, `Normalize`)

Работа с данными и метриками:

- `PIL.Image` — загрузка изображений
- `numpy`
- `sklearn.metrics` — `confusion_matrix`, `ConfusionMatrixDisplay`, `classification_report`

Визуализация:

- `matplotlib.pyplot`

Модель автоматически выбирает устройство `DEVICE`: GPU (`cuda`), если доступен, в ином случае CPU.

---

## Логика работы ноутбука по шагам

1. **Установка зависимостей**

   В начале ноутбука выполняется установка зависимостей:

   ```bash
   ! pip install -r requirements.txt
   ```

2. **Импорт и базовая настройка**

   - Импортируются модули `torch`, `torchvision`, `sklearn`, `matplotlib`, `PIL`, `numpy`.
   - Фиксируется random seed.
   - Определяется устройство исполнения модели `DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")`.

3. **Разведочный анализ структуры датасета**

   - Задаются константы:
     ```python
     DATA_DIR = "./datasets/"
     CLASSES = [...]
     SETS = ["train", "val", "test"]
     ```
   - Функция `eda(data_dir)`:
     - обходит папки классов в train/val/test;
     - считает количество изображений в каждой комбинации (набор, класс);
     - строит столбчатые диаграммы частот классов для каждого набора.

4. **Подготовка датасета и DataLoader**

   - Класс `DataSetItem(Dataset)`:
     - собирает список `(метка_класса, путь_к_файлу)`;
     - в методе `__getitem__`:
       - открывает изображение через `PIL.Image`, переводит в RGB;
       - применяет цепочку трансформаций;
       - возвращает пару (индекс класса, тензор изображения).
   - Функция `gen_data_loader(...)`:
     - сканирует подкаталоги классов;
     - создаёт объект `DataSetItem` и `DataLoader` с заданными параметрами (batch size, shuffle и т.д.).
   - Используемые трансформации:
     ```python
     transform = Compose([
         Resize((224, 224)),
         ToTensor(),
         Normalize(
             mean=[0.485, 0.456, 0.406],
             std=[0.229, 0.224, 0.225]
         ),
     ])
     ```
   - Формируются:
     ```python
     train_loader = gen_data_loader(TRAIN_DATA_DIR, transform, shuffle=True)
     val_loader   = gen_data_loader(VAL_DATA_DIR, transform)
     test_loader  = gen_data_loader(TEST_DATA_DIR, transform)
     ```

5. **Модель и оптимизатор**

   - На основе `torchvision.models.resnet18` создаётся модель-классификатор.
   - Загружаются стандартные предобученные веса.
   - Финальный полносвязный слой заменяется на слой с количеством выходов, равным числу классов.
   - Функция `gen_classifier_model(num_classes, lr=0.001)`:
     - создаёт модель;
     - переносит её на `DEVICE`;
     - создаёт оптимизатор `Adam` по тренируемым параметрам.

6. **Обучение и валидация**

   - Функция `train(model, optimizer, loader, criterion)`:
     - переводит модель в режим обучения;
     - по батчам считает loss и точность;
     - обновляет веса оптимизатором.
   - Функция `eval(model, loader, criterion)`:
     - переводит модель в режим оценки;
     - считает loss и точность на валидационной выборке.
   - Функция `train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10)`:
     - запускает цикл по эпохам;
     - на каждой эпохе выводит значения `Train Loss`, `Train Acc`, `Eval Loss`, `Eval Acc`.

7. **Оценка на тестовом наборе**

   - После обучения вычисляется точность модели на тестовом наборе данных:
     ```python
     model, _, test_acc = eval(model, test_loader)
     print(f"Точность на тестовом наборе данных: {test_acc:.4f}")
     ```
   - Функция `predict(model, loader)`:
     - возвращает списки истинных и предсказанных меток `y_true`, `y_pred`.

8. **Метрики и визуализация результатов**

   - Строится матрица ошибок:
     ```python
     cm = confusion_matrix(y_true, y_pred)
     ConfusionMatrixDisplay(cm, display_labels=CLASSES).plot(...)
     ```
   - Строится нормированная матрица ошибок:
     ```python
     cm_norm = confusion_matrix(y_true, y_pred, normalize="true")
     ```
   - Выводится отчёт по классам:
     ```python
     print(classification_report(y_true, y_pred, target_names=CLASSES))
     ```
   - Визуализируются примеры предсказаний:
     ```python
     visualize_predictions(model, test_loader.dataset, num_images=9)
     ```
     Функция случайным образом выбирает несколько изображений и выводит в сетке изображения для которых представлены пары предсказаний модели и истинных классов.

9. **Сохранение и загрузка модели**

   - Сохранение обученных весов валидированной модели:
     ```python
     os.makedirs(MODEL_DIR, exist_ok=True)
     torch.save(model.state_dict(), os.path.join(MODEL_DIR, 'resnet18_eco.pth'))
     ```
   - Загрузка в новый экземпляр модели сохранённых весов:
     ```python
     test_model = DataClassifier(len(CLASSES))
     state_dict = torch.load(os.path.join(MODEL_DIR, 'resnet18_eco.pth'), map_location=DEVICE)
     test_model.load_state_dict(state_dict)
     test_model, _, test_acc = eval(test_model, test_loader)
     print(f"Точность на тестовом наборе данных: {test_acc:.4f}")
     visualize_predictions(test_model, test_loader.dataset)
     ```

---

## Состав репозитория

Структура репозитория:

```text
.
├── datasets/                                      # данные (train/val/test, разбитые по классам)
│   ├── train/
│   ├── val/
│   └── test/
├── models/                                        # сохранённые файлы весов моделей
│   └── resnet18_eco.pth                           # веса обученной модели
├── .gitignore                                     # служебный файл git
├── README.md                                      # данный файл
├── requirements.txt                               # список зависимостей
└── Мониторинг экосистемы через IT-решения.ipynb   # основной ноутбук с решением 
```
---

## Как запустить ноутбук

1. Разместить данные в папке `datasets/` в соответствии с описанной структурой.
2. Открыть файл `Мониторинг экосистемы через IT-решения.ipynb` в Jupyter Notebook или VS Code (с необходимыми расширениями).
3. Установить зависимости:
   ```bash
   ! pip install -r requirements.txt
   ```
4. Последовательно выполнить все ячейки ноутбука сверху вниз.

---

## Текущее состояние и возможные направления развития

На текущем этапе:

- выполнен разведочный анализ структуры датасета;
- реализованы классы и функции для подготовки данных (`Dataset`, `DataLoader`);
- дообучена предобученная модель ResNet18 на пяти классах объектов;
- рассчитаны базовые метрики качества и визуализированы результаты;
- модель сохранена в файл и успешно загружается для повторного использования.

Возможные направления развития:

- расширение набора классов и объёма датасета, добавление других типов объектов окружения;
- использование более современных архитектур CV и методов повышения качества классификации;
- оборачивание модели в REST API или веб-сервис для интеграции в IT-решение мониторинга экосистемы;
- накопление и анализ предсказаний во времени и пространстве для построения карт загрязнения, комплексных экологических отчётов и научных изысканий.
